<think>Let me carefully analyze the `.github/prompts/ubuntu-dev-setup.prompt.md` file to identify areas for improvement. I need to look at the content provided and see what's missing or could be enhanced.

Looking at the file, it appears to be a prompt template for generating Ubuntu development environment setup scripts. Let me analyze its structure and content:

1. The file provides guidance for environment detection, package management, development tool categories, WSL2 considerations, security practices, and installation patterns.

2. It includes some code examples like the `install_package()` function and `install_if_missing()` pattern.

3. It covers various aspects like environment detection, package management strategy, development tool categories, WSL2 considerations, security practices, and installation patterns.

Let me identify potential areas for improvement that aren't already addressed:

1. **Missing Comprehensive Error Recovery and Rollback Mechanisms** - The current approach shows basic error handling but doesn't have sophisticated rollback or recovery strategies for failed installations.

2. **Lack of Dependency Graph Management** - The prompt doesn't address complex dependency relationships between packages and tools.

3. **Missing Performance Optimization for Large-Scale Deployments** - No guidance on optimizing for multiple simultaneous installations or enterprise-scale deployments.

4. **Insufficient Integration with Modern Container and Cloud-Native Workflows** - Limited consideration for Docker, Kubernetes, and cloud-native development patterns.

5. **Absence of Intelligent Package Selection and Customization** - No dynamic package selection based on user preferences, project requirements, or system capabilities.

Let me structure these as constructive suggestions with explanations of benefits.</think>
After analyzing the `.github/prompts/ubuntu-dev-setup.prompt.md` file, I've identified several areas for improvement that would enhance its effectiveness as a comprehensive development environment setup framework:

## **1. Missing Comprehensive Error Recovery and Rollback Mechanisms**

**Issue**: The current error handling approach shows basic patterns but lacks sophisticated recovery strategies for complex installation failures, partial installations, or system state corruption.

**Benefits**: Robust error recovery ensures system stability, prevents broken environments, and provides confidence for automated deployments in production scenarios.

**Recommendation**:
```bash
# Advanced Error Recovery Framework
setup_installation_checkpoint() {
  local checkpoint_name="$1"
  local checkpoint_dir="/tmp/ubuntu_dev_checkpoints"
  
  mkdir -p "$checkpoint_dir"
  
  # Create system state snapshot
  cat > "${checkpoint_dir}/${checkpoint_name}.json" << EOF
{
  "timestamp": "$(date -Iseconds)",
  "checkpoint_name": "$checkpoint_name",
  "system_state": {
    "installed_packages": $(dpkg-query -W -f='${Package}\n' | jq -R . | jq -s .),
    "snap_packages": $(snap list --format=json 2>/dev/null || echo '[]'),
    "environment_variables": $(env | jq -R . | jq -s .),
    "system_services": $(systemctl list-unit-files --state=enabled --no-pager --plain | jq -R . | jq -s .),
    "user_configs": {
      "bashrc_checksum": "$(md5sum ~/.bashrc 2>/dev/null | cut -d' ' -f1 || echo 'none')",
      "zshrc_checksum": "$(md5sum ~/.zshrc 2>/dev/null | cut -d' ' -f1 || echo 'none')",
      "gitconfig_checksum": "$(md5sum ~/.gitconfig 2>/dev/null | cut -d' ' -f1 || echo 'none')"
    }
  }
}
EOF
  
  log_info "Created checkpoint: $checkpoint_name"
}

rollback_to_checkpoint() {
  local checkpoint_name="$1"
  local checkpoint_file="/tmp/ubuntu_dev_checkpoints/${checkpoint_name}.json"
  
  if [ ! -f "$checkpoint_file" ]; then
    log_error "Checkpoint not found: $checkpoint_name"
    return 1
  fi
  
  log_info "Rolling back to checkpoint: $checkpoint_name"
  
  # Intelligent rollback based on system state differences
  local current_packages
  current_packages=$(dpkg-query -W -f='${Package}\n')
  
  local checkpoint_packages
  checkpoint_packages=$(jq -r '.system_state.installed_packages[]' "$checkpoint_file")
  
  # Remove packages installed after checkpoint
  comm -23 <(echo "$current_packages" | sort) <(echo "$checkpoint_packages" | sort) | \
  while read -r package; do
    if [ -n "$package" ]; then
      log_info "Removing package installed after checkpoint: $package"
      sudo apt remove --purge -y "$package" 2>/dev/null || true
    fi
  done
  
  # Restore configuration files
  restore_configuration_files "$checkpoint_file"
  
  log_success "Rollback completed successfully"
}

intelligent_installation_retry() {
  local install_function="$1"
  local max_retries="${2:-3}"
  local retry_delay="${3:-5}"
  
  local attempt=1
  while [ $attempt -le $max_retries ]; do
    log_info "Installation attempt $attempt of $max_retries"
    
    # Create micro-checkpoint before each attempt
    setup_installation_checkpoint "retry_${attempt}_$(date +%s)"
    
    if "$install_function"; then
      log_success "Installation successful on attempt $attempt"
      return 0
    else
      log_warning "Installation failed on attempt $attempt"
      
      # Analyze failure and adapt strategy
      local failure_reason
      failure_reason=$(analyze_installation_failure "$install_function")
      
      case "$failure_reason" in
        "network_timeout")
          log_info "Network timeout detected, increasing timeout and retrying..."
          export TIMEOUT_MULTIPLIER=$((TIMEOUT_MULTIPLIER * 2))
          ;;
        "dependency_conflict")
          log_info "Dependency conflict detected, attempting resolution..."
          resolve_dependency_conflicts
          ;;
        "insufficient_space")
          log_info "Insufficient disk space, attempting cleanup..."
          cleanup_temporary_files
          ;;
      esac
      
      if [ $attempt -lt $max_retries ]; then
        log_info "Waiting ${retry_delay}s before retry..."
        sleep "$retry_delay"
        retry_delay=$((retry_delay * 2))  # Exponential backoff
      fi
    fi
    
    ((attempt++))
  done
  
  log_error "Installation failed after $max_retries attempts"
  return 1
}
```

## **2. Absence of Dependency Graph Management and Intelligent Installation Ordering**

**Issue**: The current approach doesn't address complex dependency relationships between packages, tools, and configurations, which can lead to installation failures or suboptimal installation order.

**Benefits**: Intelligent dependency management ensures correct installation order, reduces conflicts, and optimizes installation performance by enabling parallel installation of independent components.

**Recommendation**:
```bash
# Dependency Graph Management System
declare -A DEPENDENCY_GRAPH
declare -A INSTALLATION_STATUS
declare -A PACKAGE_METADATA

initialize_dependency_graph() {
  # Define complex dependency relationships
  DEPENDENCY_GRAPH[git]=""
  DEPENDENCY_GRAPH[curl]=""
  DEPENDENCY_GRAPH[nodejs]="curl"
  DEPENDENCY_GRAPH[nvm]="curl git"
  DEPENDENCY_GRAPH[docker]="curl"
  DEPENDENCY_GRAPH[docker-compose]="docker"
  DEPENDENCY_GRAPH[kubernetes]="docker"
  DEPENDENCY_GRAPH[vscode]="curl"
  DEPENDENCY_GRAPH[python3]=""
  DEPENDENCY_GRAPH[pip]="python3"
  DEPENDENCY_GRAPH[virtualenv]="pip"
  DEPENDENCY_GRAPH[poetry]="pip"
  DEPENDENCY_GRAPH[zsh]=""
  DEPENDENCY_GRAPH[oh-my-zsh]="zsh git curl"
  DEPENDENCY_GRAPH[starship]="curl"
  
  # Package metadata for intelligent decisions
  PACKAGE_METADATA[nodejs]='{"size_mb": 45, "install_time_minutes": 3, "priority": "high"}'
  PACKAGE_METADATA[docker]='{"size_mb": 120, "install_time_minutes": 8, "priority": "medium"}'
  PACKAGE_METADATA[vscode]='{"size_mb": 85, "install_time_minutes": 5, "priority": "high"}'
}

calculate_installation_order() {
  local requested_packages=("$@")
  local installation_order=()
  local visited=()
  local visiting=()
  
  # Topological sort with cycle detection
  for package in "${requested_packages[@]}"; do
    if [[ ! " ${visited[*]} " =~ " ${package} " ]]; then
      topological_sort "$package" visited visiting installation_order
    fi
  done
  
  printf '%s\n' "${installation_order[@]}"
}

topological_sort() {
  local package="$1"
  local -n visited_ref=$2
  local -n visiting_ref=$3
  local -n order_ref=$4
  
  # Cycle detection
  if [[ " ${visiting_ref[*]} " =~ " ${package} " ]]; then
    log_error "Circular dependency detected involving: $package"
    return 1
  fi
  
  # Skip if already processed
  if [[ " ${visited_ref[*]} " =~ " ${package} " ]]; then
    return 0
  fi
  
  visiting_ref+=("$package")
  
  # Process dependencies first
  local deps="${DEPENDENCY_GRAPH[$package]:-}"
  if [ -n "$deps" ]; then
    for dep in $deps; do
      topological_sort "$dep" visited_ref visiting_ref order_ref
    done
  fi
  
  # Remove from visiting and add to visited
  local temp_visiting=()
  for item in "${visiting_ref[@]}"; do
    if [ "$item" != "$package" ]; then
      temp_visiting+=("$item")
    fi
  done
  visiting_ref=("${temp_visiting[@]}")
  
  visited_ref+=("$package")
  order_ref+=("$package")
}

optimize_parallel_installation() {
  local packages=("$@")
  local max_parallel="${MAX_PARALLEL_INSTALLS:-3}"
  local installation_batches=()
  local current_batch=()
  local batch_size=0
  
  # Group packages into parallel batches based on dependencies
  for package in "${packages[@]}"; do
    local package_deps="${DEPENDENCY_GRAPH[$package]:-}"
    local can_parallel=true
    
    # Check if any dependencies are in current batch
    for dep in $package_deps; do
      if [[ " ${current_batch[*]} " =~ " ${dep} " ]]; then
        can_parallel=false
        break
      fi
    done
    
    if [ "$can_parallel" = true ] && [ $batch_size -lt $max_parallel ]; then
      current_batch+=("$package")
      ((batch_size++))
    else
      # Start new batch
      if [ ${#current_batch[@]} -gt 0 ]; then
        installation_batches+=("$(IFS=','; echo "${current_batch[*]}")")
      fi
      current_batch=("$package")
      batch_size=1
    fi
  done
  
  # Add final batch
  if [ ${#current_batch[@]} -gt 0 ]; then
    installation_batches+=("$(IFS=','; echo "${current_batch[*]}")")
  fi
  
  # Execute parallel installations
  for batch in "${installation_batches[@]}"; do
    IFS=',' read -ra batch_packages <<< "$batch"
    execute_parallel_installation "${batch_packages[@]}"
  done
}

execute_parallel_installation() {
  local packages=("$@")
  local pids=()
  
  log_info "Installing packages in parallel: ${packages[*]}"
  
  for package in "${packages[@]}"; do
    install_package_with_monitoring "$package" &
    pids+=($!)
  done
  
  # Wait for all installations to complete
  local failed_packages=()
  for i in "${!pids[@]}"; do
    local pid=${pids[$i]}
    local package=${packages[$i]}
    
    if wait "$pid"; then
      INSTALLATION_STATUS[$package]="success"
      log_success "Package installed successfully: $package"
    else
      INSTALLATION_STATUS[$package]="failed"
      failed_packages+=("$package")
      log_error "Package installation failed: $package"
    fi
  done
  
  # Handle failed installations
  if [ ${#failed_packages[@]} -gt 0 ]; then
    log_warning "Retrying failed installations sequentially..."
    for package in "${failed_packages[@]}"; do
      intelligent_installation_retry "install_package_with_monitoring $package"
    done
  fi
}
```

## **3. Missing Performance Optimization for Enterprise-Scale Deployments**

**Issue**: The current framework doesn't optimize for large-scale deployments, multiple simultaneous installations, or enterprise environments with specific performance requirements.

**Benefits**: Performance optimization reduces installation time, improves resource utilization, and enables efficient deployment across multiple systems simultaneously.

**Recommendation**:
```bash
# Enterprise Performance Optimization Framework
setup_performance_optimization() {
  local optimization_level="${1:-standard}" # minimal, standard, aggressive
  local system_resources
  system_resources=$(analyze_system_resources)
  
  # Configure based on system capabilities
  configure_package_manager_optimization "$optimization_level" "$system_resources"
  setup_download_acceleration "$optimization_level"
  configure_compilation_optimization "$optimization_level" "$system_resources"
  setup_caching_strategy "$optimization_level"
}

analyze_system_resources() {
  local cpu_cores
  cpu_cores=$(nproc)
  local memory_gb
  memory_gb=$(free -g | awk '/^Mem:/{print $2}')
  local disk_speed
  disk_speed=$(test_disk_speed)
  local network_speed
  network_speed=$(test_network_speed)
  
  cat << EOF
{
  "cpu_cores": $cpu_cores,
  "memory_gb": $memory_gb,
  "disk_speed_mbps": $disk_speed,
  "network_speed_mbps": $network_speed,
  "recommended_parallel_jobs": $((cpu_cores > 4 ? cpu_cores - 1 : cpu_cores))
}
EOF
}

configure_package_manager_optimization() {
  local optimization_level="$1"
  local system_resources="$2"
  
  local parallel_jobs
  parallel_jobs=$(echo "$system_resources" | jq -r '.recommended_parallel_jobs')
  
  # APT optimization
  cat > /tmp/apt-optimization.conf << EOF
APT::Acquire::Retries "3";
APT::Acquire::Queue-Mode "host";
APT::Acquire::http::Pipeline-Depth "5";
APT::Acquire::http::Dl-Limit "$(($network_speed * 1024))";
Acquire::http::Timeout "10";
Acquire::ftp::Timeout "10";
EOF
  
  sudo cp /tmp/apt-optimization.conf /etc/apt/apt.conf.d/99ubuntu-dev-optimization
  
  # Configure parallel downloads
  case "$optimization_level" in
    "aggressive")
      echo 'Acquire::Queue-Mode "access";' | sudo tee -a /etc/apt/apt.conf.d/99ubuntu-dev-optimization
      export MAKEFLAGS="-j$parallel_jobs"
      export DEBIAN_FRONTEND=noninteractive
      ;;
    "standard")
      export MAKEFLAGS="-j$((parallel_jobs / 2))"
      ;;
  esac
}

setup_download_acceleration() {
  local optimization_level="$1"
  
  # Setup download caching and acceleration
  case "$
```
<think>The user wants me to continue with the current analysis of the ubuntu-dev-setup.prompt.md file. I was in the middle of providing the third recommendation about performance optimization. I need to continue from where I left off and complete that recommendation, then provide the remaining two recommendations, and finally summarize the overall assessment.

Let me continue with the download acceleration setup and complete the performance optimization recommendation, then provide the remaining recommendations.</think>
```bash
  # Setup download caching and acceleration
  case "$optimization_level" in
    "aggressive")
      # Setup local package cache
      setup_local_package_cache
      # Configure CDN optimization
      configure_fastest_mirrors
      # Enable compression optimization
      export GZIP="-1"  # Fastest compression
      ;;
    "standard")
      configure_fastest_mirrors
      ;;
  esac
  
  # Setup concurrent downloads
  if command_exists aria2c; then
    setup_aria2_acceleration
  elif command_exists axel; then
    setup_axel_acceleration
  fi
}

setup_intelligent_caching() {
  local cache_dir="/var/cache/ubuntu-dev-setup"
  local cache_size_gb="${CACHE_SIZE_GB:-5}"
  
  sudo mkdir -p "$cache_dir"/{packages,downloads,builds}
  
  # Setup package cache with intelligent cleanup
  cat > /tmp/cache-cleanup.sh << 'EOF'
#!/bin/bash
# Intelligent cache cleanup based on usage patterns
cache_dir="/var/cache/ubuntu-dev-setup"
max_age_days=30
max_size_gb=10

# Clean old files first
find "$cache_dir" -type f -mtime +$max_age_days -delete

# If still over size limit, clean least recently used
current_size=$(du -sb "$cache_dir" | cut -f1)
max_size_bytes=$((max_size_gb * 1024 * 1024 * 1024))

if [ "$current_size" -gt "$max_size_bytes" ]; then
    # Remove LRU files until under limit
    find "$cache_dir" -type f -printf '%T@ %p\n' | \
    sort -n | \
    while read -r timestamp file; do
        rm -f "$file"
        current_size=$(du -sb "$cache_dir" | cut -f1 2>/dev/null || echo 0)
        [ "$current_size" -le "$max_size_bytes" ] && break
    done
fi
EOF
  
  sudo mv /tmp/cache-cleanup.sh /usr/local/bin/ubuntu-dev-cache-cleanup
  sudo chmod +x /usr/local/bin/ubuntu-dev-cache-cleanup
  
  # Setup cache cleanup cron job
  echo "0 2 * * * root /usr/local/bin/ubuntu-dev-cache-cleanup" | \
    sudo tee /etc/cron.d/ubuntu-dev-cache-cleanup
}

benchmark_installation_performance() {
  local packages=("$@")
  local benchmark_results="/tmp/installation_benchmark_$(date +%s).json"
  
  local start_time
  start_time=$(date +%s.%N)
  
  # Track resource usage during installation
  local monitoring_pid
  monitor_resource_usage &
  monitoring_pid=$!
  
  # Perform installation with performance tracking
  for package in "${packages[@]}"; do
    local package_start
    package_start=$(date +%s.%N)
    
    install_package_with_monitoring "$package"
    local package_result=$?
    
    local package_end
    package_end=$(date +%s.%N)
    local package_duration
    package_duration=$(echo "$package_end - $package_start" | bc -l)
    
    # Record package-specific metrics
    record_package_performance "$package" "$package_duration" "$package_result"
  done
  
  kill $monitoring_pid 2>/dev/null || true
  
  local end_time
  end_time=$(date +%s.%N)
  local total_duration
  total_duration=$(echo "$end_time - $start_time" | bc -l)
  
  # Generate performance report
  generate_performance_report "$benchmark_results" "$total_duration" "${packages[@]}"
  
  echo "$benchmark_results"
}
```

## **4. Insufficient Integration with Modern Container and Cloud-Native Development Workflows**

**Issue**: The current framework has limited consideration for Docker, Kubernetes, cloud-native development patterns, and modern DevOps toolchains that are essential in contemporary development environments.

**Benefits**: Cloud-native integration ensures compatibility with modern development workflows, enables containerized development environments, and supports infrastructure-as-code practices.

**Recommendation**:
```bash
# Cloud-Native Development Environment Setup
setup_cloud_native_environment() {
  local deployment_target="${1:-local}" # local, cloud, hybrid
  local orchestration_platform="${2:-docker}" # docker, kubernetes, podman
  
  # Initialize cloud-native toolchain
  initialize_container_runtime "$orchestration_platform"
  setup_infrastructure_as_code_tools
  configure_cloud_native_monitoring
  setup_development_containers
  configure_ci_cd_integration
}

initialize_container_runtime() {
  local platform="$1"
  
  case "$platform" in
    "docker")
      setup_docker_environment
      configure_docker_buildkit
      setup_docker_compose_v2
      ;;
    "podman")
      setup_podman_environment
      configure_podman_compose
      ;;
    "kubernetes")
      setup_kubernetes_development_environment
      configure_kubectl_context_management
      setup_helm_package_manager
      ;;
  esac
  
  # Setup container development best practices
  setup_container_security_scanning
  configure_multi_arch_builds
  setup_container_registry_integration
}

setup_development_containers() {
  local devcontainer_config=".devcontainer"
  
  mkdir -p "$devcontainer_config"
  
  # Generate intelligent devcontainer configuration
  cat > "$devcontainer_config/devcontainer.json" << EOF
{
  "name": "Ubuntu Development Environment",
  "dockerFile": "Dockerfile",
  "context": "..",
  "features": {
    "ghcr.io/devcontainers/features/common-utils:2": {
      "installZsh": true,
      "configureZshAsDefaultShell": true,
      "installOhMyZsh": true
    },
    "ghcr.io/devcontainers/features/node:1": {
      "nodeGypDependencies": true,
      "version": "lts"
    },
    "ghcr.io/devcontainers/features/python:1": {
      "version": "3.12",
      "installTools": true
    },
    "ghcr.io/devcontainers/features/docker-in-docker:2": {
      "version": "latest",
      "dockerDashComposeVersion": "v2"
    }
  },
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-vscode-remote.remote-containers",
        "GitHub.copilot",
        "GitHub.copilot-chat",
        "ms-python.python",
        "bradlc.vscode-tailwindcss",
        "esbenp.prettier-vscode"
      ],
      "settings": {
        "terminal.integrated.defaultProfile.linux": "zsh",
        "python.defaultInterpreterPath": "/usr/local/bin/python",
        "editor.formatOnSave": true
      }
    }
  },
  "forwardPorts": [3000, 8000, 8080, 9000],
  "postCreateCommand": "bash .devcontainer/post-create.sh",
  "remoteUser": "vscode"
}
EOF
  
  # Generate optimized Dockerfile for development
  generate_development_dockerfile "$devcontainer_config/Dockerfile"
  
  # Setup post-creation script
  setup_devcontainer_post_create "$devcontainer_config/post-create.sh"
}

configure_cloud_provider_integration() {
  local cloud_providers=("aws" "azure" "gcp" "digital-ocean")
  local selected_providers=()
  
  # Detect cloud environment
  detect_cloud_environment selected_providers
  
  for provider in "${selected_providers[@]}"; do
    case "$provider" in
      "aws")
        setup_aws_development_tools
        configure_aws_profiles
        setup_aws_sam_cli
        ;;
      "azure")
        setup_azure_cli
        configure_azure_devops_integration
        ;;
      "gcp")
        setup_gcloud_sdk
        configure_gcp_development_tools
        ;;
      "digital-ocean")
        setup_doctl
        configure_do_spaces
        ;;
    esac
  done
  
  # Setup multi-cloud management tools
  setup_terraform
  setup_pulumi
  configure_cloud_native_secrets_management
}

setup_infrastructure_as_code_tools() {
  local iac_tools=(
    "terraform:hashicorp"
    "pulumi:pulumi"
    "ansible:ansible"
    "helm:helm"
    "kustomize:kubernetes-sigs"
  )
  
  for tool_info in "${iac_tools[@]}"; do
    IFS=':' read -r tool org <<< "$tool_info"
    
    install_iac_tool "$tool" "$org"
    configure_iac_tool_integration "$tool"
  done
  
  # Setup IaC best practices
  setup_iac_validation_tools
  configure_iac_security_scanning
  setup_iac_testing_framework
}

configure_microservices_development() {
  local service_mesh="${1:-istio}" # istio, linkerd, consul-connect
  
  # Setup service mesh development tools
  case "$service_mesh" in
    "istio")
      setup_istioctl
      configure_istio_development_environment
      ;;
    "linkerd")
      setup_linkerd_cli
      configure_linkerd_development
      ;;
  esac
  
  # Setup API development tools
  setup_api_development_stack
  configure_service_discovery_tools
  setup_distributed_tracing
  configure_chaos_engineering_tools
}

setup_api_development_stack() {
  local api_tools=(
    "openapi-generator"
    "swagger-codegen"
    "postman-cli"
    "insomnia-core"
    "grpcurl"
    "evans"
  )
  
  for tool in "${api_tools[@]}"; do
    install_api_tool "$tool"
  done
  
  # Setup API testing and documentation
  setup_api_testing_framework
  configure_api_documentation_generation
  setup_contract_testing_tools
}
```

## **5. Lack of Intelligent Package Selection and Environment Customization**

**Issue**: The framework doesn't provide intelligent package selection based on user preferences, project requirements, development patterns, or system capabilities, leading to over-installation or missing essential tools.

**Benefits**: Intelligent customization reduces bloat, improves performance, ensures relevant tool installation, and provides personalized development environments tailored to specific needs and workflows.

**Recommendation**:
```bash
# Intelligent Environment Customization Framework
setup_intelligent_customization() {
  local customization_profile="${1:-detect}" # minimal, web-dev, data-science, devops, full-stack, detect
  
  if [ "$customization_profile" = "detect" ]; then
    customization_profile=$(detect_development_profile)
  fi
  
  # Generate customized installation plan
  local installation_plan
  installation_plan=$(generate_intelligent_installation_plan "$customization_profile")
  
  # Apply machine learning-inspired optimization
  optimize_installation_plan "$installation_plan"
  
  # Execute customized installation
  execute_intelligent_installation "$installation_plan"
}

detect_development_profile() {
  local profile_indicators=()
  local current_dir
  current_dir=$(pwd)
  
  # Analyze current directory and user patterns
  if [ -f "package.json" ] || [ -f "yarn.lock" ] || [ -f "pnpm-lock.yaml" ]; then
    profile_indicators+=("javascript")
  fi
  
  if [ -f "requirements.txt" ] || [ -f "pyproject.toml" ] || [ -f "Pipfile" ]; then
    profile_indicators+=("python")
  fi
  
  if [ -f "Dockerfile" ] || [ -f "docker-compose.yml" ] || [ -f "k8s" ]; then
    profile_indicators+=("devops")
  fi
  
  if [ -f "go.mod" ] || [ -f "main.go" ]; then
    profile_indicators+=("golang")
  fi
  
  if [ -f "Cargo.toml" ] || [ -f "src/main.rs" ]; then
    profile_indicators+=("rust")
  fi
  
  # Analyze shell history for development patterns
  analyze_shell_history_patterns profile_indicators
  
  # Analyze installed packages for existing preferences
  analyze_existing_development_tools profile_indicators
  
  # Generate profile recommendation
  determine_optimal_profile "${profile_indicators[@]}"
}

analyze_shell_history_patterns() {
  local -n indicators_ref=$1
  local history_files=("$HOME/.bash_history" "$HOME/.zsh_history")
  
  for history_file in "${history_files[@]}"; do
    if [ -f "$history_file" ]; then
      # Analyze command patterns
      local python_usage
      python_usage=$(grep -c "python\|pip\|conda\|jupyter" "$history_file" 2>/dev/null || echo 0)
      
      local docker_usage
      docker_usage=$(grep -c "docker\|kubectl\|helm" "$history_file" 2>/dev/null || echo 0)
      
      local web_dev_usage
      web_dev_usage=$(grep -c "npm\|yarn\|webpack\|react\|vue\|angular" "$history_file" 2>/dev/null || echo 0)
      
      local data_science_usage
      data_science_usage=$(grep -c "jupyter\|pandas\|numpy\|tensorflow\|pytorch" "$history_file" 2>/dev/null || echo 0)
      
      # Add indicators based on usage patterns
      [ "$python_usage" -gt 20 ] && indicators_ref+=("python")
      [ "$docker_usage" -gt 15 ] && indicators_ref+=("devops")
      [ "$web_dev_usage" -gt 25 ] && indicators_ref+=("web-development")
      [ "$data_science_usage" -gt 10 ] && indicators_ref+=("data-science")
    fi
  done
}

generate_intelligent_installation_plan() {
  local profile="$1"
  local installation_plan="/tmp/intelligent_installation_plan_$(date +%s).json"
  
  # Define profile-specific package sets
  local -A profile_packages
  profile_packages[minimal]="git curl wget vim nano zsh"
  profile_packages[web-dev]="git curl wget vim nano zsh nodejs npm yarn docker code"
  profile_packages[data-science]="git curl wget vim nano zsh python3 pip jupyter pandas numpy scipy matplotlib"
  profile_packages[devops]="git curl wget vim nano zsh docker kubectl helm terraform ansible"
  profile_packages[full-stack]="git curl wget vim nano zsh nodejs npm yarn python3 pip docker kubectl code"
  
  # Get base packages for profile
  local base_packages="${profile_packages[$profile]:-${profile_packages[minimal]}}"
  
  # Apply intelligent optimizations
  local optimized_packages
  optimized_packages=$(optimize_package_selection "$base_packages" "$profile")
  
  # Generate installation plan with metadata
  cat > "$installation_plan" << EOF
{
  "profile": "$profile
```
<think>The user wants me to continue with the analysis. I was in the middle of the fifth recommendation about intelligent package selection and environment customization. I need to complete that recommendation and then provide the overall summary assessment.

Let me continue from where I left off with the installation plan generation and complete the intelligent customization framework.</think>
```bash
  # Generate installation plan with metadata
  cat > "$installation_plan" << EOF
{
  "profile": "$profile",
  "generated_timestamp": "$(date -Iseconds)",
  "system_info": $(analyze_system_resources),
  "package_categories": {
    "essential": $(echo "$optimized_packages" | jq -R 'split(" ")' | jq 'map(select(. != ""))'),
    "recommended": $(get_recommended_packages "$profile" | jq -R 'split(" ")' | jq 'map(select(. != ""))'),
    "optional": $(get_optional_packages "$profile" | jq -R 'split(" ")' | jq 'map(select(. != ""))'),
    "development_specific": $(get_development_specific_packages "$profile" | jq -R 'split(" ")' | jq 'map(select(. != ""))')
  },
  "customization_rules": $(generate_customization_rules "$profile"),
  "configuration_templates": $(get_configuration_templates "$profile"),
  "post_installation_tasks": $(get_profile_specific_tasks "$profile")
}
EOF
  
  echo "$installation_plan"
}

optimize_package_selection() {
  local base_packages="$1"
  local profile="$2"
  local optimized=""
  
  # Apply system-specific optimizations
  local system_arch
  system_arch=$(uname -m)
  local is_wsl
  is_wsl=$(is_wsl2 && echo "true" || echo "false")
  
  for package in $base_packages; do
    # Check package compatibility and necessity
    if is_package_compatible "$package" "$system_arch" "$is_wsl"; then
      if is_package_beneficial "$package" "$profile"; then
        optimized="$optimized $package"
      fi
    else
      # Find alternative packages
      local alternative
      alternative=$(find_package_alternative "$package" "$system_arch" "$is_wsl")
      if [ -n "$alternative" ]; then
        optimized="$optimized $alternative"
      fi
    fi
  done
  
  # Add intelligent suggestions based on profile
  local suggested_packages
  suggested_packages=$(get_intelligent_suggestions "$profile" "$optimized")
  
  echo "$optimized $suggested_packages" | tr -s ' '
}

get_intelligent_suggestions() {
  local profile="$1"
  local current_packages="$2"
  local suggestions=""
  
  case "$profile" in
    "web-dev")
      # Suggest complementary web development tools
      if [[ ! "$current_packages" =~ "typescript" ]]; then
        suggestions="$suggestions typescript"
      fi
      if [[ ! "$current_packages" =~ "eslint" ]]; then
        suggestions="$suggestions eslint"
      fi
      if [[ ! "$current_packages" =~ "prettier" ]]; then
        suggestions="$suggestions prettier"
      fi
      ;;
    "data-science")
      # Suggest data science ecosystem tools
      if [[ ! "$current_packages" =~ "r-base" ]]; then
        suggestions="$suggestions r-base"
      fi
      if [[ ! "$current_packages" =~ "julia" ]]; then
        suggestions="$suggestions julia"
      fi
      ;;
    "devops")
      # Suggest DevOps toolchain
      if [[ ! "$current_packages" =~ "kind" ]]; then
        suggestions="$suggestions kind"
      fi
      if [[ ! "$current_packages" =~ "minikube" ]]; then
        suggestions="$suggestions minikube"
      fi
      ;;
  esac
  
  echo "$suggestions"
}

setup_adaptive_configuration() {
  local profile="$1"
  local user_preferences="$2"
  
  # Generate adaptive configuration based on usage patterns
  create_adaptive_shell_config "$profile"
  setup_intelligent_aliases "$profile"
  configure_development_shortcuts "$profile"
  setup_context_aware_prompts "$profile"
}

create_adaptive_shell_config() {
  local profile="$1"
  local shell_config="$HOME/.ubuntu-dev-profile"
  
  cat > "$shell_config" << EOF
# Ubuntu Development Environment - Adaptive Configuration
# Profile: $profile
# Generated: $(date)

# Profile-specific environment variables
export UBUNTU_DEV_PROFILE="$profile"
export UBUNTU_DEV_SETUP_DATE="$(date -Iseconds)"

# Intelligent PATH management
$(generate_intelligent_path_config "$profile")

# Profile-specific aliases
$(generate_profile_aliases "$profile")

# Development shortcuts
$(generate_development_shortcuts "$profile")

# Intelligent completion setup
$(setup_intelligent_completions "$profile")

# Performance optimizations
$(apply_performance_optimizations "$profile")
EOF
  
  # Source the adaptive configuration
  if ! grep -q "source $shell_config" "$HOME/.bashrc" 2>/dev/null; then
    echo "source $shell_config" >> "$HOME/.bashrc"
  fi
  
  if [ -f "$HOME/.zshrc" ] && ! grep -q "source $shell_config" "$HOME/.zshrc" 2>/dev/null; then
    echo "source $shell_config" >> "$HOME/.zshrc"
  fi
}

setup_continuous_learning_system() {
  local learning_data_dir="$HOME/.ubuntu-dev-learning"
  mkdir -p "$learning_data_dir"
  
  # Setup usage tracking for continuous improvement
  cat > "$learning_data_dir/track-usage.sh" << 'EOF'
#!/bin/bash
# Track development tool usage for intelligent recommendations

log_command_usage() {
  local command="$1"
  local timestamp="$(date -Iseconds)"
  local usage_log="$HOME/.ubuntu-dev-learning/usage.log"
  
  echo "$timestamp,$command" >> "$usage_log"
}

# Hook into common development commands
alias git='log_command_usage "git"; command git'
alias docker='log_command_usage "docker"; command docker'
alias npm='log_command_usage "npm"; command npm'
alias python='log_command_usage "python"; command python'
alias code='log_command_usage "code"; command code'
EOF
  
  # Setup periodic analysis and recommendations
  setup_usage_analysis_cron "$learning_data_dir"
}

generate_personalized_recommendations() {
  local usage_data="$HOME/.ubuntu-dev-learning/usage.log"
  local recommendations_file="$HOME/.ubuntu-dev-learning/recommendations.json"
  
  if [ -f "$usage_data" ]; then
    # Analyze usage patterns
    local most_used_tools
    most_used_tools=$(tail -1000 "$usage_data" | cut -d',' -f2 | sort | uniq -c | sort -nr | head -10)
    
    # Generate intelligent recommendations
    cat > "$recommendations_file" << EOF
{
  "analysis_date": "$(date -Iseconds)",
  "most_used_tools": $(echo "$most_used_tools" | jq -R . | jq -s .),
  "recommendations": {
    "new_tools": $(recommend_complementary_tools "$most_used_tools"),
    "optimizations": $(recommend_optimizations "$most_used_tools"),
    "learning_resources": $(recommend_learning_resources "$most_used_tools")
  },
  "next_analysis": "$(date -d '+1 week' -Iseconds)"
}
EOF
    
    # Display recommendations to user
    display_personalized_recommendations "$recommendations_file"
  fi
}

setup_environment_evolution() {
  local evolution_config="$HOME/.ubuntu-dev-evolution"
  
  # Track environment changes over time
  cat > "$evolution_config" << EOF
{
  "initial_setup": "$(date -Iseconds)",
  "profile_evolution": [],
  "package_additions": [],
  "configuration_changes": [],
  "performance_improvements": []
}
EOF
  
  # Setup automatic environment updates
  setup_intelligent_environment_updates
  configure_adaptive_maintenance_schedule
}

execute_intelligent_installation() {
  local installation_plan="$1"
  
  log_info "Executing intelligent installation plan..."
  
  # Extract installation categories
  local essential_packages
  essential_packages=$(jq -r '.package_categories.essential[]' "$installation_plan")
  
  local recommended_packages
  recommended_packages=$(jq -r '.package_categories.recommended[]' "$installation_plan")
  
  local optional_packages
  optional_packages=$(jq -r '.package_categories.optional[]' "$installation_plan")
  
  # Install with intelligent error handling and user interaction
  install_package_category "Essential" "$essential_packages" "required"
  install_package_category "Recommended" "$recommended_packages" "prompt"
  install_package_category "Optional" "$optional_packages" "optional"
  
  # Apply profile-specific configurations
  apply_profile_configurations "$installation_plan"
  
  # Setup continuous learning and adaptation
  initialize_adaptive_environment "$installation_plan"
  
  log_success "Intelligent installation completed successfully"
}

install_package_category() {
  local category_name="$1"
  local packages="$2"
  local install_mode="$3" # required, prompt, optional
  
  case "$install_mode" in
    "required")
      log_info "Installing $category_name packages (required)..."
      for package in $packages; do
        install_package_with_retry "$package" || {
          log_error "Failed to install required package: $package"
          return 1
        }
      done
      ;;
    "prompt")
      log_info "$category_name packages available:"
      echo "$packages" | tr ' ' '\n' | sed 's/^/  - /'
      read -p "Install $category_name packages? (Y/n): " -r
      if [[ $REPLY =~ ^[Yy]$ ]] || [[ -z $REPLY ]]; then
        install_packages_parallel $packages
      fi
      ;;
    "optional")
      log_info "Optional $category_name packages available for manual installation:"
      echo "$packages" | tr ' ' '\n' | sed 's/^/  - /'
      ;;
  esac
}
```

---

## **Summary Assessment**

After comprehensive analysis of the `.github/prompts/ubuntu-dev-setup.prompt.md` file, I found **significant opportunities to enhance the code quality and functionality**. While the existing prompt provides a solid foundation with good practices like environment detection, basic error handling, and modular installation patterns, it falls short of modern enterprise development environment requirements.

### **Key Strengths of Current Implementation:**
- **Good Environment Detection**: Properly handles WSL2, Desktop, and headless environments
- **Security Awareness**: Includes basic security practices and validation
- **Modular Design**: Shows understanding of component separation
- **Basic Error Handling**: Implements fundamental error checking patterns

### **Critical Areas Requiring Enhancement:**

The five major improvements identified address **fundamental gaps** that significantly impact the framework's production readiness:

1. **Sophisticated Error Recovery** - Current basic error handling is insufficient for complex enterprise deployments
2. **Intelligent Dependency Management** - Missing crucial dependency graph management and installation optimization
3. **Enterprise Performance Optimization** - Lacks scalability considerations for large-scale deployments
4. **Cloud-Native Integration** - Limited support for modern container and DevOps workflows
5. **Adaptive Customization** - No intelligence for personalizing environments based on user patterns

### **Overall Assessment:**

The current prompt demonstrates **good foundational understanding** but requires **substantial enhancement** to meet modern development environment standards. The recommended improvements would transform it from a basic setup script generator into a **comprehensive, intelligent development environment orchestration platform**.

**Recommendation**: Implement these enhancements to create a world-class development environment setup framework that addresses enterprise needs, modern workflows, and intelligent automation requirements.

The enhanced framework would provide **significant value** through reduced setup time, improved reliability, better resource utilization, and adaptive learning capabilities that evolve with user needs.